name: performance-benchmark

on:
  pull_request:
    paths:
      - 'lib/**'
      - 'test/perf/**'
      - '.github/workflows/perf-benchmark.yml'
  push:
    branches: [main, develop]
  workflow_dispatch:

jobs:
  benchmark:
    runs-on: macos-latest
    timeout-minutes: 30

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Flutter
        uses: subosito/flutter-action@v2
        with:
          channel: stable
          cache: true

      - name: Install dependencies
        run: flutter pub get

      - name: Generate code
        run: flutter pub run build_runner build --delete-conflicting-outputs

      - name: Run performance benchmarks
        run: |
          # Create output directory for metrics
          mkdir -p test_results

          # Run performance tests
          flutter test test/perf --no-pub --reporter=json > test_results/perf_test_raw.json || true

      - name: Process benchmark results
        run: |
          # Parse and format results for GitHub output
          if [ -f test_results/perf_test_raw.json ]; then
            echo "Performance test results:"
            cat test_results/perf_test_raw.json | jq -r '.result | select(.testID | contains("perf")) | "\(.testID): \(.status)"' || echo "No performance tests found"
          else
            echo "No performance test results found"
          fi

      - name: Generate performance metrics for CI dashboard
        run: |
          # Generate performance_metrics.json for CI dashboard consumption
          cat > test_results/performance_metrics.json << 'EOF'
          {
            "run_id": "${{ github.run_id }}",
            "run_number": "${{ github.run_number }}",
            "sha": "${{ github.sha }}",
            "ref": "${{ github.ref_name }}",
            "event": "${{ github.event_name }}",
            "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
            "metrics": []
          }
          EOF

          # Extract metrics from test results if available
          if [ -f test_results/perf_test_raw.json ]; then
            # Create a temporary script to parse and append metrics
            cat > /tmp/parse_metrics.py << 'PYEOF'
          import json
          import sys

          # Read the template
          with open('test_results/performance_metrics.json', 'r') as f:
              metrics_data = json.load(f)

          # Read test results
          with open('test_results/perf_test_raw.json', 'r') as f:
              test_results = json.load(f)

          # Extract performance test results
          for result in test_results.get('result', []):
              test_id = result.get('testID', '')
              if 'perf' in test_id.lower():
                  status = result.get('status', 'unknown')
                  time_ms = result.get('time', 0) * 1000  # Convert to milliseconds
                  hidden = result.get('hidden', {})
                  skipped = hidden.get('skipped', False)

                  metric = {
                      "name": test_id,
                      "status": status,
                      "duration_ms": round(time_ms, 2),
                      "skipped": skipped
                  }

                  # Add metadata if test failed
                  if status == 'fail' and 'error' in result:
                      metric['error'] = result['error']

                  metrics_data['metrics'].append(metric)

          # Add summary
          total = len(metrics_data['metrics'])
          passed = sum(1 for m in metrics_data['metrics'] if m['status'] == 'pass')
          failed = sum(1 for m in metrics_data['metrics'] if m['status'] == 'fail')
          skipped_count = sum(1 for m in metrics_data['metrics'] if m['skipped'])

          metrics_data['summary'] = {
              "total": total,
              "passed": passed,
              "failed": failed,
              "skipped": skipped_count,
              "success_rate": round((passed / total * 100) if total > 0 else 0, 2)
          }

          # Write the final metrics file
          with open('test_results/performance_metrics.json', 'w') as f:
              json.dump(metrics_data, f, indent=2)

          print(f"Generated metrics: {passed}/{total} tests passed")
          PYEOF

            python3 /tmp/parse_metrics.py
          else
            echo "No test results available, creating empty metrics file"
          fi

          # Display the metrics
          echo "Generated performance_metrics.json:"
          cat test_results/performance_metrics.json

      - name: Upload performance test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-test-results
          path: test_results/
          retention-days: 30

      - name: Upload performance metrics for CI dashboard
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-metrics
          path: test_results/performance_metrics.json
          retention-days: 90

      - name: Download baseline (if exists)
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            try {
              const response = await github.rest.repos.getContent({
                owner: context.repo.owner,
                repo: context.repo.repo,
                path: 'test/perf/baselines.json',
                ref: 'main'
              });
              const content = Buffer.from(response.data.content, 'base64').toString();
              fs.writeFileSync('test/perf/baselines.json', content);
              console.log('Baseline downloaded successfully');
            } catch (error) {
              console.log('No baseline found, this will be a baseline run');
            }

      - name: Generate performance report
        if: always()
        run: |
          # Generate a summary report from the test results
          echo "## Performance Test Results" > test_results/performance_summary.md
          echo "" >> test_results/performance_summary.md
          echo "**Commit:** ${{ github.sha }}" >> test_results/performance_summary.md
          echo "**Branch:** ${{ github.ref_name }}" >> test_results/performance_summary.md
          echo "" >> test_results/performance_summary.md

          # Extract test results
          if [ -f test_results/perf_test_raw.json ]; then
            echo "### Test Results" >> test_results/performance_summary.md
            echo "" >> test_results/performance_summary.md
            cat test_results/perf_test_raw.json | jq -r '
              .result | select(.testID | contains("perf")) |
              "| \(.testID) | \(.status) | \(.hidden?.skipped // "N/A") |"
            ' >> test_results/performance_summary.md || echo "No results to display"
          fi

      - name: Comment PR with performance results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let report = '## ðŸ“Š Performance Benchmark Results\n\n';

            try {
              const summary = fs.readFileSync('test_results/performance_summary.md', 'utf8');
              report += summary;
            } catch (error) {
              report += 'No performance summary available.\n';
            }

            report += `\n\n**Workflow Run:** https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('ðŸ“Š Performance Benchmark Results')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: report,
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: report,
              });
            }

      - name: Check for performance regression
        if: always()
        run: |
          # Run regression detection script
          # Fails if performance degrades by more than 20%
          bash scripts/check_performance_regression.sh \
            --baseline test/perf/baselines.json \
            --results test_results/perf_test_raw.json \
            --threshold 20.0 \
            --verbose
